import pandas as pd
import os
import torch
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
import sys

# Setup import path for utils
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.text_processing import preprocess_text

# === 1. Setup Path (à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸²à¸—à¸µà¹ˆà¸­à¸¢à¸¹à¹ˆà¹„à¸Ÿà¸¥à¹Œ) ===
# à¸«à¸²à¸•à¸³à¹à¸«à¸™à¹ˆà¸‡à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™ à¹à¸¥à¹‰à¸§à¸–à¸­à¸¢à¸à¸¥à¸±à¸šà¹„à¸›à¸«à¸² backend
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
BACKEND_DIR = os.path.dirname(CURRENT_DIR)
DATA_DIR = os.path.join(BACKEND_DIR, "data")
MODELS_DIR = os.path.join(BACKEND_DIR, "models")
OUTPUT_MODEL_DIR = os.path.join(MODELS_DIR, "my_thai_news_model")

# à¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¹€à¸à¹‡à¸šà¹‚à¸¡à¹€à¸”à¸¥à¸–à¹‰à¸²à¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¸¡à¸µ
if not os.path.exists(MODELS_DIR):
    os.makedirs(MODELS_DIR)

# === 2. Config Device (à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸²à¸à¸²à¸£à¹Œà¸”à¸ˆà¸­) ===
# à¹€à¸Šà¹‡à¸„à¸§à¹ˆà¸²à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡à¸¡à¸µ GPU à¸­à¸°à¹„à¸£à¹ƒà¸«à¹‰à¹ƒà¸Šà¹‰à¸šà¹‰à¸²à¸‡ (Mac M1/M2 à¹ƒà¸Šà¹‰ mps)
if torch.cuda.is_available():
    device = "cuda"
    print("ðŸš€ Using GPU (CUDA)")
elif torch.backends.mps.is_available():
    device = "mps"
    print("ðŸŽ Using Mac GPU (MPS) - à¹à¸£à¸‡à¹à¸™à¹ˆà¸™à¸­à¸™!")
else:
    device = "cpu"
    print("ðŸ¢ Using CPU (Might be slow)")

# === 3. Load & Prepare Data (à¹€à¸•à¸£à¸µà¸¢à¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥) ===
print(f"ðŸ“‚ Loading Data from: {DATA_DIR}")
csv_path = os.path.join(DATA_DIR, '11.agnews_thai_test_hard.csv')

try:
    df = pd.read_csv(csv_path)
except FileNotFoundError:
    print("âŒ à¹„à¸¡à¹ˆà¹€à¸ˆà¸­à¹„à¸Ÿà¸¥à¹Œ CSV! à¸à¸£à¸¸à¸“à¸²à¹€à¸Šà¹‡à¸„à¸§à¹ˆà¸²à¹„à¸Ÿà¸¥à¹Œ 11.agnews_thai_test_hard.csv à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™ folder 'backend/data' à¹à¸¥à¹‰à¸§à¸«à¸£à¸·à¸­à¸¢à¸±à¸‡")
    exit()

# à¸£à¸§à¸¡à¸«à¸±à¸§à¸‚à¹‰à¸­à¸à¸±à¸šà¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸‚à¹ˆà¸²à¸§à¹€à¸‚à¹‰à¸²à¸”à¹‰à¸§à¸¢à¸à¸±à¸™ à¹à¸¥à¸°à¸—à¸³ Preprocessing
df['text'] = (df['headline'] + " " + df['body']).apply(preprocess_text)
label_map = {'World': 0, 'Business': 1, 'SciTech': 2}
df['label'] = df['topic'].map(label_map)

# à¹à¸šà¹ˆà¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ Train 80% / Validation 20%
train_texts, val_texts, train_labels, val_labels = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)

# à¹à¸›à¸¥à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹ƒà¸«à¹‰à¹€à¸›à¹‡à¸™ Format à¸‚à¸­à¸‡ Hugging Face Dataset
train_dataset = Dataset.from_pandas(pd.DataFrame({'text': train_texts, 'label': train_labels}))
val_dataset = Dataset.from_pandas(pd.DataFrame({'text': val_texts, 'label': val_labels}))

# === 4. Tokenizer (à¸•à¸±à¸§à¸•à¸±à¸”à¸„à¸³) ===
MODEL_NAME = "airesearch/wangchanberta-base-att-spm-uncased"
print(f"â¬‡ï¸ Downloading Tokenizer: {MODEL_NAME}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

def tokenize_function(examples):
    # à¸•à¸±à¸”à¸„à¸³à¹à¸¥à¸°à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™à¸•à¸±à¸§à¹€à¸¥à¸‚ (Padding à¹ƒà¸«à¹‰à¹€à¸—à¹ˆà¸²à¸à¸±à¸™à¸—à¸µà¹ˆ 128)
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)

print("âš™ï¸ Tokenizing data...")
tokenized_train = train_dataset.map(tokenize_function, batched=True)
tokenized_val = val_dataset.map(tokenize_function, batched=True)

# === 5. Model Setup (à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥) ===
print("â¬‡ï¸ Downloading Model...")
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)
model.to(device) # à¸ªà¹ˆà¸‡à¹‚à¸¡à¹€à¸”à¸¥à¹„à¸›à¸—à¸µà¹ˆ GPU/MPS

# === 6. Training Arguments (à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸²à¸à¸²à¸£à¹€à¸—à¸£à¸™) ===
# à¸•à¸£à¸‡à¸™à¸µà¹‰à¸„à¸·à¸­à¸ˆà¸¸à¸”à¸—à¸µà¹ˆà¹à¸à¹‰à¸šà¸±à¹Šà¸ evaluation_strategy -> eval_strategy
training_args = TrainingArguments(
    output_dir="./results_temp",    # à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸Šà¸±à¹ˆà¸§à¸„à¸£à¸²à¸§
    num_train_epochs=3,             # à¹€à¸—à¸£à¸™ 3 à¸£à¸­à¸š (à¸–à¹‰à¸²à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡à¸£à¹‰à¸­à¸™à¸¥à¸”à¹€à¸«à¸¥à¸·à¸­ 1-2 à¹„à¸”à¹‰)
    per_device_train_batch_size=8,  # à¸‚à¸™à¸²à¸” Batch (à¸–à¹‰à¸² RAM à¹„à¸¡à¹ˆà¸žà¸­à¹ƒà¸«à¹‰à¸¥à¸”à¹€à¸«à¸¥à¸·à¸­ 4)
    per_device_eval_batch_size=8,
    
    eval_strategy="epoch",          # <--- âœ… à¹à¸à¹‰à¹„à¸‚à¹à¸¥à¹‰à¸§: à¹ƒà¸«à¹‰à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸œà¸¥à¸—à¸¸à¸à¸ˆà¸š Epoch
    
    save_strategy="no",             # à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¹€à¸‹à¸Ÿ Checkpoint à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡à¸—à¸²à¸‡ (à¹€à¸›à¸¥à¸·à¸­à¸‡à¸—à¸µà¹ˆ)
    learning_rate=2e-5,             # à¸­à¸±à¸•à¸£à¸²à¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
)

# === 7. Start Training (à¹€à¸£à¸´à¹ˆà¸¡à¹€à¸—à¸£à¸™) ===
print("ðŸš€ Start Training... (à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸™à¸µà¹‰à¹ƒà¸Šà¹‰à¹€à¸§à¸¥à¸²à¸ªà¸±à¸à¸žà¸±à¸ à¹€à¸•à¸£à¸µà¸¢à¸¡à¸à¸²à¹à¸Ÿà¸£à¸­à¹„à¸”à¹‰à¹€à¸¥à¸¢ â˜•)")
trainer.train()

# === 8. Save Model (à¸šà¸±à¸™à¸—à¸¶à¸à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œ) ===
print(f"ðŸ’¾ Saving model to: {OUTPUT_MODEL_DIR}")
model.save_pretrained(OUTPUT_MODEL_DIR)
tokenizer.save_pretrained(OUTPUT_MODEL_DIR)

print("ðŸŽ‰ Training Complete! WangchanBERTa is ready for action.")